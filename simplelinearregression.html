<html lang="" dir="ltr">

<head>
  <meta charset="utf-8">
  <title>ML PRACS</title>
</head>

<body>

    <pre>
        import pandas as pd
        import numpy as np
        import math
        import operator
        import matplotlib.pyplot as plt
        %matplotlib inline
        from google.colab import files
        uploaded = files.upload()
        import io

        data = pd.read_csv(io.BytesI0(uploaded['headbrain.csv']))
        print(data.head())

        # collecting x & y
        X = data['Head Size(cm3) '].values
        Y = data['Brain Weight(grams)'].values
        # calculate mean of x & y using an inbuilt numpy method mean()
        mean_x = np.mean(X)
        mean_y = np.mean(Y)
        # total no.of input values
        m = len(X)

        # using the formula to calculate m & c
        numer = 0
        denom = 0
        for i in range(m):
            numer += (X[i] - mean_x) * (Y[i] - mean_y)
            denom += (X[i] - mean_x) ** 2
        m = numer / denom
        c = mean_y - (m * mean_x)
        print (f'm = {m} \nc = {c}')

        # plotting values and regression line
        max_x = np.max(X) + 100
        min_x = np.min(Y) - 100
        # calculating line values x and y
        x = np.linspace (min_x, max_x, 100)
        y = c + m * x
        plt.plot(x, y, color='#58b970', label='Regression Line')
        plt.scatter(X, Y, c='#ef5423', label='data points')
        plt.xlabel('Head Size in cm')
        plt.ylabel('Brain Weight in grams')
        plt.legend()
        plt.show()


        import numpy as np
        import pandas as pd

        def analytical_linear_regression(X, y):
            X = np.hstack((np.ones((X.shape[0], 1)), X))
            theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)
            return theta

        def gradient_descent(X, y, alpha=0.01, iterations=1000):
            m = len(y)
            theta = np.zeros((X.shape[1], 1))
            for i in range(iterations):
                h = X.dot(theta)
                error = h - y
                gradient = X.T.dot(error) / m
                theta -= alpha * gradient
            return theta

        # Example usage:
        data = pd.read_csv('data.csv')
        X = data.iloc[:, :-1].values
        y = data.iloc[:, -1].values.reshape(-1, 1)

        # Analytical method:
        theta_analytical = analytical_linear_regression(X, y)
        print(f"Analytical method: {theta_analytical.ravel()}")

        # Gradient descent method:
        X = (X - X.mean()) / X.std()
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        theta_gradient_descent = gradient_descent(X, y)
        print(f"Gradient descent method: {theta_gradient_descent.ravel()}")



    </pre>

</body>
</html>