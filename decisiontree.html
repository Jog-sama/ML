<html lang="" dir="ltr">

<head>
  <meta charset="utf-8">
  <title>ML PRACS</title>
</head>

<body>

    <pre>
        import numpy as np
        import pandas as pd
        
        class DecisionTree:
            def __init__(self, max_depth=5):
                self.max_depth = max_depth
        
            def fit(self, X, y):
                self.n_classes_ = len(np.unique(y))
                self.tree_ = self._grow_tree(X, y)
        
            def predict(self, X):
                return [self._predict(inputs) for inputs in X]
        
            def _best_split(self, X, y):
                m = y.size
                if m <= 1:
                    return None, None
                num_parent = [np.sum(y == c) for c in range(self.n_classes_)]
                best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent)
                best_idx, best_thr = None, None
                for idx in range(X.shape[1]):
                    thresholds, classes = zip(*sorted(zip(X[:, idx], y)))
                    num_left = [0] * self.n_classes_
                    num_right = num_parent.copy()
                    for i in range(1, m):
                        c = classes[i - 1]
                        num_left[c] += 1
                        num_right[c] -= 1
                        gini_left = 1.0 - sum(
                            (num_left[x] / i) ** 2 for x in range(self.n_classes_)
                        )
                        gini_right = 1.0 - sum(
                            (num_right[x] / (m - i)) ** 2 for x in range(self.n_classes_)
                        )
                        gini = (i * gini_left + (m - i) * gini_right) / m
                        if thresholds[i] == thresholds[i - 1]:
                            continue
                        if gini < best_gini:
                            best_gini = gini
                            best_idx = idx
                            best_thr = (thresholds[i] + thresholds[i - 1]) / 2
                return best_idx, best_thr
        
            def _grow_tree(self, X, y, depth=0):
                num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]
                predicted_class = np.argmax(num_samples_per_class)
                node = Node(
                    predicted_class=predicted_class,
                    num_samples=y.size,
                    num_samples_per_class=num_samples_per_class,
                )
        
                if depth < self.max_depth:
                    idx, thr = self._best_split(X, y)
                    if idx is not None:
                        indices_left = X[:, idx] < thr
                        X_left, y_left = X[indices_left], y[indices_left]
                        X_right, y_right = X[~indices_left], y[~indices_left]
                        node.feature_index = idx
                        node.threshold = thr
                        node.left_child = self._grow_tree(X_left, y_left, depth + 1)
                        node.right_child = self._grow_tree(X_right, y_right, depth + 1)
                return node
        
            def _predict(self, inputs):
                node = self.tree_
                while node.left_child:
                    if inputs[node.feature_index] < node.threshold:
                        node = node.left_child
                    else:
                        node = node.right_child
                return node.predicted_class
        
        
        class Node:
            def __init__(self, *, predicted_class=None, num_samples=None,
                         num_samples_per_class=None, feature_index=None,
                         threshold=None, left_child=None, right_child=None):
                self.predicted_class = predicted_class
                self.num_samples = num_samples
                self.num_samples_per_class = num_samples_per_class
                self.feature_index = feature_index
                self.threshold = threshold
                self.left_child = left_child
                self.right_child = right_child
        
        # Example usage:
        data = pd.read_csv('data.csv')
        X_train = data.iloc[:, :-1].values
        y_train = data.iloc[:, -1].values.reshape(-1, 1)
        tree_model = DecisionTree(max_depth=5)
        tree_model.fit(X_train, y_train)
        X_test = np.array([[5.5], [6.7], [7.8]])
        y_pred_test = tree_model.predict(X_test)
        print(y_pred_test)
        Heres how to use the code:
        
        First create a DecisionTree object with the maximum depth of the tree as a parameter.
        Then call the fit method of the object with the training data X_train and y_train.
        Finally call the predict method of the object with the test data X_test to get the predicted labels.



        import numpy as np
        import pandas as pd

        class Node:
            def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):
                self.feature = feature
                self.threshold = threshold
                self.left = left
                self.right = right
                self.value = value

        class DecisionTree:
            def __init__(self, min_samples_split=2, max_depth=100):
                self.min_samples_split = min_samples_split
                self.max_depth = max_depth

            def fit(self, X, y):
                self.n_classes_ = len(np.unique(y))
                self.n_features_ = X.shape[1]
                self.tree_ = self._grow_tree(X, y)

            def predict(self, X):
                return [self._predict(inputs) for inputs in X]

            def _best_split(self, X, y):
                m = y.size
                if m <= self.min_samples_split:
                    return None, None
                num_parent = [np.sum(y == c) for c in range(self.n_classes_)]
                best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent)
                best_idx, best_thr = None, None
                for idx in range(self.n_features_):
                    thresholds, classes = zip(*sorted(zip(X[:, idx], y)))
                    num_left = [0] * self.n_classes_
                    num_right = num_parent.copy()
                    for i in range(1, m):
                        c = classes[i - 1]
                        num_left[c] += 1
                        num_right[c] -= 1
                        gini_left = 1.0 - sum(
                            (num_left[x] / i) ** 2 for x in range(self.n_classes_)
                        )
                        gini_right = 1.0 - sum(
                            (num_right[x] / (m - i)) ** 2 for x in range(self.n_classes_)
                        )
                        gini = (i * gini_left + (m - i) * gini_right) / m
                        if thresholds[i] == thresholds[i - 1]:
                            continue
                        if gini < best_gini:
                            best_gini = gini
                            best_idx = idx
                            best_thr = (thresholds[i] + thresholds[i - 1]) / 2
                return best_idx, best_thr

            def _grow_tree(self, X, y, depth=0):
                num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]
                predicted_class = np.argmax(num_samples_per_class)
                node = Node(
                    feature=None,
                    threshold=None,
                    value=predicted_class,
                )

                if depth < self.max_depth:
                    idx, thr = self._best_split(X, y)
                    if idx is not None:
                        indices_left = X[:, idx] < thr
                        X_left, y_left = X[indices_left], y[indices_left]
                        X_right, y_right = X[~indices_left], y[~indices_left]
                        node.feature = idx
                        node.threshold = thr
                        node.left = self._grow_tree(X_left, y_left, depth + 1)
                        node.right = self._grow_tree(X_right, y_right, depth + 1)
                return node

            def _predict(self, inputs):
                node = self.tree_
                while node.value is None:
                    if inputs[node.feature] < node.threshold:
                        node = node.left
                    else:
                        node = node.right
                return node.value

        # Load the dataset
        data = pd.read_csv('iris.csv')

        # Split the dataset into features and target variable
        X_train = data.iloc[:, :-1].values
        y_train = data.iloc[:, -1].values.reshape(-1, 1)

        # Train the decision tree classifier
        clf = DecisionTree(min_samples_split=5)
        clf.fit(X_train, y_train)

        # Predict the target variable for the test set
        X_test = np.array([[5.5, 3.5, 1.3, 0.2], [6.7, 3.0, 5.2, 2.3], [7.8, 3.8, 6.7, 2.2]])
        y_pred_test = clf.predict(X_test)
        print(y_pred_test)
        Heres how to use the code:

        First create a DecisionTree object with the minimum number of samples required to split a node and maximum depth of the tree as parameters.
        Then call the fit method of the object with the training data X_train and y_train.
        Finally call the predict method of the object with the test data X_test to get the

    </pre>

</body>
</html>